{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depression in Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benthompson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/benthompson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benthompson/anaconda/envs/pi/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/benthompson/anaconda/envs/pi/lib/python3.4/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import nltk library\n",
    "import nltk; nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "# import stopword libraries\n",
    "nltk.download('stopwords'); from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "# import other libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# import word embedding library\n",
    "#import glove_helper\n",
    "\n",
    "# import helper libraries\n",
    "import collections\n",
    "from common import utils, vocabulary\n",
    "\n",
    "#display multiple results per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#export models\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in tweets\n",
    "df = pd.DataFrame.from_csv('../depression_tweets.csv', header=None, parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add index\n",
    "df = df.reset_index()\n",
    "\n",
    "#set column names\n",
    "df.columns = ['date','tweet_id', 'handle', 'id', 'tweet', 'language', 'device', 'notes', 'notes_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>handle</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "      <th>device</th>\n",
       "      <th>notes</th>\n",
       "      <th>notes_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-05 19:14:48</td>\n",
       "      <td>981973445616525312</td>\n",
       "      <td>Haldol</td>\n",
       "      <td>816793117785542656</td>\n",
       "      <td>Currently I am on 150 mg of hydroxyzine for in...</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-05 19:14:48</td>\n",
       "      <td>981973444723064832</td>\n",
       "      <td>Rick O</td>\n",
       "      <td>3192532759</td>\n",
       "      <td>Integrated behavioral health for POLICE. Treat...</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-05 19:14:47</td>\n",
       "      <td>981973443988996096</td>\n",
       "      <td>olivia üßùüèΩ‚Äç‚ôÄÔ∏è„Éú„Çπ</td>\n",
       "      <td>1321438920</td>\n",
       "      <td>RT @DevinnJay: I won‚Äôt allow depression to fuc...</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-05 19:14:47</td>\n",
       "      <td>981973443154505728</td>\n",
       "      <td>LeFrenchNeuropsy</td>\n",
       "      <td>2887994266</td>\n",
       "      <td>RT @LePsylab: For science ! Un questionnaire p...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-05 19:14:45</td>\n",
       "      <td>981973435705421826</td>\n",
       "      <td>GEEZ</td>\n",
       "      <td>311289251</td>\n",
       "      <td>I lost my brova I fell deep in depression!</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date            tweet_id            handle  \\\n",
       "0 2018-04-05 19:14:48  981973445616525312            Haldol   \n",
       "1 2018-04-05 19:14:48  981973444723064832            Rick O   \n",
       "2 2018-04-05 19:14:47  981973443988996096    olivia üßùüèΩ‚Äç‚ôÄÔ∏è„Éú„Çπ   \n",
       "3 2018-04-05 19:14:47  981973443154505728  LeFrenchNeuropsy   \n",
       "4 2018-04-05 19:14:45  981973435705421826              GEEZ   \n",
       "\n",
       "                   id                                              tweet  \\\n",
       "0  816793117785542656  Currently I am on 150 mg of hydroxyzine for in...   \n",
       "1          3192532759  Integrated behavioral health for POLICE. Treat...   \n",
       "2          1321438920  RT @DevinnJay: I won‚Äôt allow depression to fuc...   \n",
       "3          2887994266  RT @LePsylab: For science ! Un questionnaire p...   \n",
       "4           311289251         I lost my brova I fell deep in depression!   \n",
       "\n",
       "  language               device  notes  notes_2  \n",
       "0       en   Twitter for iPhone    NaN      NaN  \n",
       "1       en   Twitter for iPhone    NaN      NaN  \n",
       "2       en   Twitter for iPhone    NaN      NaN  \n",
       "3       fr   Twitter Web Client    NaN      NaN  \n",
       "4       en  Twitter for Android    NaN      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530773"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how man non-distinct tweets\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter to english only\n",
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484663"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many tweets now\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".                    1073\n",
       "Aiden Hatfield        460\n",
       "Ÿã                     445\n",
       "Documentary           313\n",
       "In Music We Trust     312\n",
       "Name: handle, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#any users w/lots of tweets that might skew model?\n",
    "#not any that seem too high\n",
    "df['handle'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186362"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many distinct tweets\n",
    "len(df.tweet.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make distinct tweets the df\n",
    "df = pd.DataFrame(df.tweet.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export sample to check quality\n",
    "# pd.options.display.max_colwidth = 1000\n",
    "# df_sample = df.sample(n=100)\n",
    "# df_sample.to_csv('../sample_100_depression_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweets    Y‚Äôall be using mental illness as a way to justify your bitchy behavior and it‚Äôs honestly a no from fucking me. Depr‚Ä¶ https://t.co/YzlhNSV9RA\n",
       "Name: 45055, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look up specific tweet\n",
    "pd.options.display.max_colwidth = 10000\n",
    "df.iloc[45055]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create column on 1's\n",
    "x = [1]\n",
    "x = x * len(df)\n",
    "df['target'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Currently I am on 150 mg of hydroxyzine for in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Integrated behavioral health for POLICE. Treat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @DevinnJay: I won‚Äôt allow depression to fuc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I lost my brova I fell deep in depression!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @peachesfrfr: so there i am  depression all...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  target\n",
       "0  Currently I am on 150 mg of hydroxyzine for in...       1\n",
       "1  Integrated behavioral health for POLICE. Treat...       1\n",
       "2  RT @DevinnJay: I won‚Äôt allow depression to fuc...       1\n",
       "3         I lost my brova I fell deep in depression!       1\n",
       "4  RT @peachesfrfr: so there i am  depression all...       1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in tweets\n",
    "df_2 = pd.DataFrame.from_csv('../random_tweets_2018-06-15.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RT @EmmanuelMacron: France, Germany, and the UK regret the U.S. decision to leave the JCPOA. The nuclear non-proliferation regime is at sta‚Ä¶</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @glossoIogy: Gemini: bitch\\nCancer: blocked \\nGemini: unblock me I need to tell you something \\nCancer: what \\nGemini: bitch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @JDiamond1: Iranian President Hassan Rouhani: Iran will abide by its JCPOA commitments despite US withdrawal. Says agreement now between‚Ä¶</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @PaulLee85: @Redheaded_Jenn @EGh69 Jenn, is my white knight. But apparently she isn‚Äôt American, since she‚Äôs more interested I. What goes‚Ä¶</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rereading it now too I realize I was being hard on myself but MAN it'S REALLY NICE TO GET GOOD RECEPTION ON STUFF Y‚Ä¶ https://t.co/lUlaFc1ztu</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [RT @EmmanuelMacron: France, Germany, and the UK regret the U.S. decision to leave the JCPOA. The nuclear non-proliferation regime is at sta‚Ä¶, RT @glossoIogy: Gemini: bitch\n",
       "Cancer: blocked \n",
       "Gemini: unblock me I need to tell you something \n",
       "Cancer: what \n",
       "Gemini: bitch, RT @JDiamond1: Iranian President Hassan Rouhani: Iran will abide by its JCPOA commitments despite US withdrawal. Says agreement now between‚Ä¶, RT @PaulLee85: @Redheaded_Jenn @EGh69 Jenn, is my white knight. But apparently she isn‚Äôt American, since she‚Äôs more interested I. What goes‚Ä¶, Rereading it now too I realize I was being hard on myself but MAN it'S REALLY NICE TO GET GOOD RECEPTION ON STUFF Y‚Ä¶ https://t.co/lUlaFc1ztu]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at data\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96206"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many\n",
    "len(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#give index\n",
    "df_2 = df_2.reset_index()\n",
    "\n",
    "#give column name\n",
    "df_2.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78329"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many distinct tweets\n",
    "len(df_2.tweets.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make dataframe of unique\n",
    "df_2 = pd.DataFrame(df_2.tweets.unique())\n",
    "\n",
    "#give column name\n",
    "df_2.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make all tweets lowercase\n",
    "df_2['tweets'] = df_2['tweets'].str.lower()\n",
    "df_2.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt @emmanuelmacron: france, germany, and the u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt @glossoiogy: gemini: bitch\\ncancer: blocked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt @jdiamond1: iranian president hassan rouhan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt @paullee85: @redheaded_jenn @egh69 jenn, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rereading it now too i realize i was being har...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets\n",
       "0  rt @emmanuelmacron: france, germany, and the u...\n",
       "1  rt @glossoiogy: gemini: bitch\\ncancer: blocked...\n",
       "2  rt @jdiamond1: iranian president hassan rouhan...\n",
       "3  rt @paullee85: @redheaded_jenn @egh69 jenn, is...\n",
       "4  rereading it now too i realize i was being har..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>rt @sophxthompson: it's #mentalhealthawareness...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>rt @duumb: commercial: 2 out of 3 people suffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>rt @depresseddarth: nobody respects me anymore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>rt @heavybagofbones: bitches who got full scor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>rt @kirebe16: i suffer from anxiety but lately...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7128</th>\n",
       "      <td>rt @evanedinger: after realising last week my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>rt @depresseddarth: ‚Äúsorry, wrong galaxy‚Äù http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>rt @notrllyhere: sometimes you just b depresse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>rt @brysontiller: 1. i was depressed before i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>rt @depressionnote: what people need to unders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10206</th>\n",
       "      <td>rt @youknowmamakay: sad? pray\\nover thinking? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>thank you wil wheaton for sharing your life jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10401</th>\n",
       "      <td>this girl was online talking about dealing wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10464</th>\n",
       "      <td>@lantis_jiles mfs really be that depressed???üíÄ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10859</th>\n",
       "      <td>why are sanders, booker, gillibrand, et al. pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10934</th>\n",
       "      <td>i am determined to get laundry done today beca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11031</th>\n",
       "      <td>rt @lukepast: like... we all know the girl who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>depressed üí©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16004</th>\n",
       "      <td>rt @depresseddarth: the circle of life https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17343</th>\n",
       "      <td>rt @cheating: my playlist is so crazy got u th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17620</th>\n",
       "      <td>rt @alxbmfrd: when ur usually the comedian of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17688</th>\n",
       "      <td>rt @ketaminh: so i made my daughter this omele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19403</th>\n",
       "      <td>rt @depresseddarth: shut up and take my money ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19927</th>\n",
       "      <td>rt @starlighttmikey: stop romanticising suicid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22933</th>\n",
       "      <td>rt @yeonkiminj: yoonmin au \\n\\nyoongi was beyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23771</th>\n",
       "      <td>rt @hitunesthealbum: this song cured my depres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24129</th>\n",
       "      <td>i'm kind of depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24862</th>\n",
       "      <td>rt @depresseddarth: true love means never havi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25030</th>\n",
       "      <td>you speak on mental health like u know a singl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27773</th>\n",
       "      <td>rt @depresseddarth: cardi b does sound effects...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58265</th>\n",
       "      <td>i‚Äôm high key depressed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58338</th>\n",
       "      <td>rt @depressionnote: today i feel\\n\\n‚Ä¢ abandone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58443</th>\n",
       "      <td>rt @depressionnote: it doesn‚Äôt matter what rac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58912</th>\n",
       "      <td>rt @camhnews: 'kate spade‚Äôs death proves depre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58920</th>\n",
       "      <td>rt @stevenspohn: the most successful people i'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59240</th>\n",
       "      <td>rt @itsrjhill: i‚Äôm so depressed yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59501</th>\n",
       "      <td>rt @nikkilipstick: ‚ú®üñ§üéÄwhen you finally get a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60036</th>\n",
       "      <td>@freshempire how come there's not a depressed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61216</th>\n",
       "      <td>i feel like i just hit straight random depress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61320</th>\n",
       "      <td>rt @v_nooguyen: one of my professors wrote a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62642</th>\n",
       "      <td>rt @supremeshana_: religion doesn't cure depre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62678</th>\n",
       "      <td>rt @drgnkiller: i like how he starts with ‚Äúi‚Äôm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63941</th>\n",
       "      <td>rt @a_joseph1616: \"thanks to psychedelic drugs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64232</th>\n",
       "      <td>the intensity of my depression tends to strike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64423</th>\n",
       "      <td>rt @sidayaofficial: causes of insomnia: \\n\\nre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66879</th>\n",
       "      <td>rt @sierracheyanne_: i hope this shit makes me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68210</th>\n",
       "      <td>rt @athariyyah_: it's actually bare upsetting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69386</th>\n",
       "      <td>rt @tonysraptorsofc: i wish there was one day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69664</th>\n",
       "      <td>rt @leelamad: @nazeeahmed no run means i am de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71810</th>\n",
       "      <td>off work finally yes now time to go home and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71814</th>\n",
       "      <td>rt @asapyams: very depressed writing in lower ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72638</th>\n",
       "      <td>@19chelsea94 i quit my job recently to focus o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73265</th>\n",
       "      <td>that‚Äôs called depression lol https://t.co/fhmu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73912</th>\n",
       "      <td>rt @spacebrat: its funny how u get depressed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74166</th>\n",
       "      <td>rt @itzwikipedia: too much homework can cause ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75058</th>\n",
       "      <td>i‚Äôm so tired of being a worthless piece of shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75765</th>\n",
       "      <td>trump's america wants to get rid of coverage f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76404</th>\n",
       "      <td>rt @rosyfuck: @bbzaria @aljaminbrydie omg did ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76529</th>\n",
       "      <td>this is for those, who are sad, frustrated, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77783</th>\n",
       "      <td>shoutout to being depressed and malnourished f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets\n",
       "1241   rt @sophxthompson: it's #mentalhealthawareness...\n",
       "1894   rt @duumb: commercial: 2 out of 3 people suffe...\n",
       "1900   rt @depresseddarth: nobody respects me anymore...\n",
       "2814   rt @heavybagofbones: bitches who got full scor...\n",
       "6739   rt @kirebe16: i suffer from anxiety but lately...\n",
       "7128   rt @evanedinger: after realising last week my ...\n",
       "7300   rt @depresseddarth: ‚Äúsorry, wrong galaxy‚Äù http...\n",
       "8701   rt @notrllyhere: sometimes you just b depresse...\n",
       "9750   rt @brysontiller: 1. i was depressed before i ...\n",
       "10122  rt @depressionnote: what people need to unders...\n",
       "10206  rt @youknowmamakay: sad? pray\\nover thinking? ...\n",
       "10258  thank you wil wheaton for sharing your life jo...\n",
       "10401  this girl was online talking about dealing wit...\n",
       "10464     @lantis_jiles mfs really be that depressed???üíÄ\n",
       "10859  why are sanders, booker, gillibrand, et al. pu...\n",
       "10934  i am determined to get laundry done today beca...\n",
       "11031  rt @lukepast: like... we all know the girl who...\n",
       "13996                                        depressed üí©\n",
       "16004  rt @depresseddarth: the circle of life https:/...\n",
       "17343  rt @cheating: my playlist is so crazy got u th...\n",
       "17620  rt @alxbmfrd: when ur usually the comedian of ...\n",
       "17688  rt @ketaminh: so i made my daughter this omele...\n",
       "19403  rt @depresseddarth: shut up and take my money ...\n",
       "19927  rt @starlighttmikey: stop romanticising suicid...\n",
       "22933  rt @yeonkiminj: yoonmin au \\n\\nyoongi was beyo...\n",
       "23771  rt @hitunesthealbum: this song cured my depres...\n",
       "24129                              i'm kind of depressed\n",
       "24862  rt @depresseddarth: true love means never havi...\n",
       "25030  you speak on mental health like u know a singl...\n",
       "27773  rt @depresseddarth: cardi b does sound effects...\n",
       "...                                                  ...\n",
       "58265                            i‚Äôm high key depressed.\n",
       "58338  rt @depressionnote: today i feel\\n\\n‚Ä¢ abandone...\n",
       "58443  rt @depressionnote: it doesn‚Äôt matter what rac...\n",
       "58912  rt @camhnews: 'kate spade‚Äôs death proves depre...\n",
       "58920  rt @stevenspohn: the most successful people i'...\n",
       "59240                 rt @itsrjhill: i‚Äôm so depressed yo\n",
       "59501  rt @nikkilipstick: ‚ú®üñ§üéÄwhen you finally get a h...\n",
       "60036  @freshempire how come there's not a depressed ...\n",
       "61216  i feel like i just hit straight random depress...\n",
       "61320  rt @v_nooguyen: one of my professors wrote a t...\n",
       "62642  rt @supremeshana_: religion doesn't cure depre...\n",
       "62678  rt @drgnkiller: i like how he starts with ‚Äúi‚Äôm...\n",
       "63941  rt @a_joseph1616: \"thanks to psychedelic drugs...\n",
       "64232  the intensity of my depression tends to strike...\n",
       "64423  rt @sidayaofficial: causes of insomnia: \\n\\nre...\n",
       "66879  rt @sierracheyanne_: i hope this shit makes me...\n",
       "68210  rt @athariyyah_: it's actually bare upsetting ...\n",
       "69386  rt @tonysraptorsofc: i wish there was one day ...\n",
       "69664  rt @leelamad: @nazeeahmed no run means i am de...\n",
       "71810  off work finally yes now time to go home and l...\n",
       "71814  rt @asapyams: very depressed writing in lower ...\n",
       "72638  @19chelsea94 i quit my job recently to focus o...\n",
       "73265  that‚Äôs called depression lol https://t.co/fhmu...\n",
       "73912  rt @spacebrat: its funny how u get depressed a...\n",
       "74166  rt @itzwikipedia: too much homework can cause ...\n",
       "75058  i‚Äôm so tired of being a worthless piece of shi...\n",
       "75765  trump's america wants to get rid of coverage f...\n",
       "76404  rt @rosyfuck: @bbzaria @aljaminbrydie omg did ...\n",
       "76529  this is for those, who are sad, frustrated, al...\n",
       "77783  shoutout to being depressed and malnourished f...\n",
       "\n",
       "[96 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for tweets that use depression\n",
    "df_2[(df_2['tweets'].str.contains('depressed') | df_2['tweets'].str.contains('depression'))]\n",
    "\n",
    "#drop them\n",
    "df_2.drop(df_2[(df_2.tweets.str.contains('depressed')) | (df_2.tweets.str.contains('depression'))].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78233"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recheck length\n",
    "len(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export to check quality\n",
    "# df_2_sample = df_2.sample(n=100)\n",
    "# df_2_sample.to_csv('../sample_100_random_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#column of 0's\n",
    "x = 0\n",
    "x = x * len(df_2)\n",
    "\n",
    "df_2['target'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#balance classes\n",
    "df_3 = df.sample(n=len(df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine dfs\n",
    "df = pd.concat([df_3,df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156466"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'rt @ techreview : a neural network can detect depression and mania in bipolar subjects by analyzing how they hold and tap on their smartphone‚Ä¶ '\n"
     ]
    }
   ],
   "source": [
    "#preprocess tweets\n",
    "example_text=\"\"\"'RT @techreview: A neural network can \n",
    "detect depression and mania in bipolar subjects \n",
    "by analyzing how they hold and tap on their smartphone‚Ä¶'\"\"\"\n",
    "\n",
    "# tokenize\n",
    "def tokenize_text(input_text):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    input_text: a string representing an \n",
    "    individual review\n",
    "        \n",
    "    Returns:\n",
    "    input_token: a list containing stemmed \n",
    "    tokens, with punctutations removed, for \n",
    "    an individual review\n",
    "        \n",
    "    \"\"\"\n",
    "    input_tokens=[]\n",
    "        \n",
    "    # Split sentence\n",
    "    sents=sent_tokenize(input_text)\n",
    "            \n",
    "    # Split word\n",
    "    for sent in sents:\n",
    "        input_tokens+=TreebankWordTokenizer().tokenize(sent)\n",
    "        \n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "# canonicalize\n",
    "def canonicalize_tokens(input_tokens):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    input_tokens: a list containing tokenized \n",
    "    tokens for an individual review\n",
    "    \n",
    "    Returns:\n",
    "    input_tokens: a list containing canonicalized \n",
    "    tokens for an individual review\n",
    "    \n",
    "    \"\"\"\n",
    "    input_tokens=utils.canonicalize_words(input_tokens)\n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "# preprocessor \n",
    "def preprocessor(raw_text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    raw_text: a string representing an\n",
    "    individual review\n",
    "    \n",
    "    Returns:\n",
    "    preprocessed_text: a string representing \n",
    "    a preprocessed individual review\n",
    "    \n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    tokens=tokenize_text(raw_text)\n",
    "    \n",
    "    # canonicalize\n",
    "    canonical_tokens=canonicalize_tokens(tokens)\n",
    "    \n",
    "    # rejoin string\n",
    "    preprocessed_text=(\" \").join(canonical_tokens) \n",
    "    return preprocessed_text\n",
    "\n",
    "# example data\n",
    "#input_tokens=tokenize_text(example_text)\n",
    "#print(input_tokens)\n",
    "\n",
    "#canonical_tokens=canonicalize_tokens(input_tokens)\n",
    "#print(canonical_tokens)\n",
    "\n",
    "preprocessed_text=preprocessor(example_text) \n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sklearn stopwords: 318\n",
      "number of nltk stopwords: 179\n",
      "number of total stopwords: 387\n"
     ]
    }
   ],
   "source": [
    "# examine stopwords\n",
    "\n",
    "# sklearn stopwords (frozenset)\n",
    "sklearn_stopwords=stop_words.ENGLISH_STOP_WORDS\n",
    "print(\"number of sklearn stopwords: %d\" %(len(sklearn_stopwords)))\n",
    "#print(sklearn_stopwords)\n",
    "\n",
    "# nltk stopwords (list)\n",
    "nltk_stopwords=stopwords.words(\"english\")\n",
    "print(\"number of nltk stopwords: %d\" %(len(nltk_stopwords)))\n",
    "#print(nltk_stopwords)\n",
    "\n",
    "# combined sklearn, nltk, other stopwords (set)\n",
    "total_stopwords=set(list(sklearn_stopwords.difference(set(nltk_stopwords)))+nltk_stopwords)\n",
    "\n",
    "other_stopwords=[\"DG\", \"DGDG\", \"@\", \"rt\", \"'rt\", \"'\", \":\", \"depression\", \"depressed\"]\n",
    "for w in other_stopwords:\n",
    "    total_stopwords.add(w)\n",
    "    \n",
    "print(\"number of total stopwords: %d\" %(len(total_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['techreview', 'neural', 'network', 'detect', 'mania', 'bipolar', 'subjects', 'analyzing', 'hold', 'tap', 'smartphone‚Ä¶']\n"
     ]
    }
   ],
   "source": [
    "#look at review w/o stop words\n",
    "new_review = []\n",
    "for i in preprocessed_text.split():\n",
    "    if i in total_stopwords:\n",
    "        continue\n",
    "    else:\n",
    "        new_review.append(i)\n",
    "        \n",
    "print(new_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test set size: 125221, 31245\n",
      "\n",
      "example:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-1fd7c2b8a016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# examine train set examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"example:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweet: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pi/lib/python3.4/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   1898\u001b[0m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1900\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3557)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3240)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:8564)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:8508)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 5"
     ]
    }
   ],
   "source": [
    "#split into test, train before sampling to belance\n",
    "# using recoded labels\n",
    "#create train, test data\n",
    "df['is_train'] = np.random.uniform(0,1, len(df)) <= .8\n",
    "\n",
    "train_data, test_data = df[df['is_train'] == True], df[df['is_train'] == False]\n",
    "\n",
    "# examine train, test shapes\n",
    "print(\"train, test set size: %d, %d\" %(len(train_data), len(test_data))) # train_data: 129023, test_data: 32256\n",
    "print(\"\")\n",
    "\n",
    "# examine train set examples\n",
    "print(\"example:\")\n",
    "print(\"tweet: %s\" %(train_data.get_value(5,'tweets')))\n",
    "print(\"label: %s\" %(train_data.get_value(5,'target')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    62645\n",
       "1    62576\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check class balance\n",
    "train_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:\n",
      "tweet: @akcgoros I send you hugs. It's really hard when you want to create something but depression won't let you  so you get depressed.\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"example:\")\n",
    "print(\"tweet: %s\" %(train_data.get_value(30,'tweets')))\n",
    "print(\"label: %s\" %(train_data.get_value(30,'target')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build tf-idf model\n",
    "vec=TfidfVectorizer(preprocessor=preprocessor, ngram_range=(1,3), stop_words=total_stopwords, max_features=10000)\n",
    "vec_train_data=vec.fit_transform(train_data['tweets']) \n",
    "vec_test_data=vec.transform(test_data['tweets']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression f1 score: 0.816\n",
      "logistic regression accuracy score: 0.816\n",
      "logistic regression confusion matrix:\n",
      "[[13426  2162]\n",
      " [ 3584 12073]]\n"
     ]
    }
   ],
   "source": [
    "# train Logistic Regression\n",
    "logit=LogisticRegression(penalty='l2')\n",
    "logit.fit(vec_train_data, train_data['target'])\n",
    "pred_labels=logit.predict(vec_test_data)\n",
    "    \n",
    "# assess model\n",
    "f1=f1_score(test_data['target'], pred_labels, average=\"weighted\") \n",
    "accuracy=accuracy_score(test_data['target'], pred_labels)\n",
    "confusion=confusion_matrix(test_data['target'], pred_labels)\n",
    "print(\"logistic regression f1 score: %.3f\" %(f1))\n",
    "print(\"logistic regression accuracy score: %.3f\" %(accuracy))\n",
    "print(\"logistic regression confusion matrix:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights of words that predict depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>naps</th>\n",
       "      <td>5.409314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicidal</th>\n",
       "      <td>5.412190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>battling</th>\n",
       "      <td>5.694694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cures</th>\n",
       "      <td>5.743700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>5.814887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nap</th>\n",
       "      <td>5.818789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>5.941904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postpartum</th>\n",
       "      <td>6.029521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffering</th>\n",
       "      <td>6.318642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tropical</th>\n",
       "      <td>6.431674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffer</th>\n",
       "      <td>6.460533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mental</th>\n",
       "      <td>6.496839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mentalhealth</th>\n",
       "      <td>6.514196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seasonal</th>\n",
       "      <td>7.403417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imwtclothing</th>\n",
       "      <td>7.465921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crippling</th>\n",
       "      <td>7.830916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cure</th>\n",
       "      <td>9.504229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicide</th>\n",
       "      <td>9.847079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cured</th>\n",
       "      <td>10.993831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxiety</th>\n",
       "      <td>16.609908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Weights of words that predict depression\n",
       "naps                                          5.409314\n",
       "suicidal                                      5.412190\n",
       "battling                                      5.694694\n",
       "cures                                         5.743700\n",
       "clinical                                      5.814887\n",
       "nap                                           5.818789\n",
       "sadness                                       5.941904\n",
       "postpartum                                    6.029521\n",
       "suffering                                     6.318642\n",
       "tropical                                      6.431674\n",
       "suffer                                        6.460533\n",
       "mental                                        6.496839\n",
       "mentalhealth                                  6.514196\n",
       "seasonal                                      7.403417\n",
       "imwtclothing                                  7.465921\n",
       "crippling                                     7.830916\n",
       "cure                                          9.504229\n",
       "suicide                                       9.847079\n",
       "cured                                        10.993831\n",
       "anxiety                                      16.609908"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get top words\n",
    "#look at top 5 weights for each class\n",
    "#get coefficients for all features\n",
    "coef_sq = logit.coef_\n",
    "\n",
    "#get index of top 5 absolute values for each class\n",
    "weight_indx = np.argsort(coef_sq)[:, -20:]\n",
    "\n",
    "#flatten so can use to look up wieghts\n",
    "weight_indx = weight_indx.flatten()\n",
    "\n",
    "#get coefficients based on index\n",
    "weights = coef_sq[:, weight_indx]\n",
    " \n",
    "#get words that match weights based on index\n",
    "vocab = np.array(vec.get_feature_names())[weight_indx]\n",
    "\n",
    "# make table\n",
    "df = pd.DataFrame({'Weights of words that predict depression': weights[0]}\n",
    "                  , index=vocab)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of class 0 and 1:  [[ 0.43736519  0.56263481]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights of words in sample Journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>store</th>\n",
       "      <td>-1.171633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>-0.457797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interaction</th>\n",
       "      <td>-0.419084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderful</th>\n",
       "      <td>-0.374465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strange</th>\n",
       "      <td>0.326909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weird</th>\n",
       "      <td>0.392027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.923644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>makes feel</th>\n",
       "      <td>1.083485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>1.660564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>makes</th>\n",
       "      <td>1.798566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>2.465471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Weights of words in sample Journal\n",
       "store                                 -1.171633\n",
       "sure                                  -0.457797\n",
       "interaction                           -0.419084\n",
       "wonderful                             -0.374465\n",
       "strange                                0.326909\n",
       "weird                                  0.392027\n",
       "today                                  0.923644\n",
       "makes feel                             1.083485\n",
       "going                                  1.660564\n",
       "makes                                  1.798566\n",
       "feel                                   2.465471"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try to make up an example journal\n",
    "journal = \"\"\"Today was wonderful. I had a strange interaction at the store. \n",
    "The cashier seemed irratated. I'm not sure what's going on but it makes me feel weird\"\"\"\n",
    "\n",
    "#score test journal\n",
    "vec_test_example=vec.transform([journal]) \n",
    "print(\"probability of class 0 and 1: \",logit.predict_proba(vec_test_example))\n",
    "\n",
    "#get words and weights from test journal\n",
    "word_idx = np.nonzero(vec_test_example)[1]\n",
    "vocab = np.array(vec.get_feature_names())[word_idx]\n",
    "weights = coef_sq[:, word_idx]\n",
    "df = pd.DataFrame({'Weights of words in sample Journal': weights[0]}\n",
    "                  , index=vocab)\n",
    "df.sort_values(by='Weights of words in sample Journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_exported_model']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export tfidf model\n",
    "tfidf_file = 'tfidf_exported_model'\n",
    "joblib.dump(vec, tfidf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_regression_model']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export logistic regression\n",
    "logistic_regression_file = 'logistic_regression_model'\n",
    "joblib.dump(logit, logistic_regression_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of class 0 and 1:  [[ 0.43736519  0.56263481]]\n"
     ]
    }
   ],
   "source": [
    "#test out exported models against prev sample journal\n",
    "loaded_tfidf = joblib.load('tfidf_exported_model')\n",
    "loaded_lr = joblib.load('logistic_regression_model')\n",
    "\n",
    "#score test journal\n",
    "export_test_example=loaded_tfidf.transform([journal]) \n",
    "print(\"probability of class 0 and 1: \",loaded_lr.predict_proba(export_test_example))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi",
   "language": "python",
   "name": "pi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
