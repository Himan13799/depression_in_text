{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benthompson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/benthompson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benthompson/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/benthompson/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import nltk library\n",
    "import nltk; nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "# import stopword libraries\n",
    "nltk.download('stopwords'); from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "# import other libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# import word embedding library\n",
    "#import glove_helper\n",
    "\n",
    "# import helper libraries\n",
    "import collections\n",
    "from common import utils, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in tweets\n",
    "df = pd.DataFrame.from_csv('../depression_tweets.csv', header=None, parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add index\n",
    "df = df.reset_index()\n",
    "\n",
    "#set column names\n",
    "df.columns = ['date','tweet_id', 'handle', 'id', 'tweet', 'language', 'device', 'notes', 'notes_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>handle</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "      <th>device</th>\n",
       "      <th>notes</th>\n",
       "      <th>notes_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-05 19:14:48</td>\n",
       "      <td>981973445616525312</td>\n",
       "      <td>Haldol</td>\n",
       "      <td>816793117785542656</td>\n",
       "      <td>Currently I am on 150 mg of hydroxyzine for in...</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-05 19:14:48</td>\n",
       "      <td>981973444723064832</td>\n",
       "      <td>Rick O</td>\n",
       "      <td>3192532759</td>\n",
       "      <td>Integrated behavioral health for POLICE. Treat...</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-05 19:14:47</td>\n",
       "      <td>981973443988996096</td>\n",
       "      <td>olivia üßùüèΩ‚Äç‚ôÄÔ∏è„Éú„Çπ</td>\n",
       "      <td>1321438920</td>\n",
       "      <td>RT @DevinnJay: I won‚Äôt allow depression to fuc...</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-05 19:14:47</td>\n",
       "      <td>981973443154505728</td>\n",
       "      <td>LeFrenchNeuropsy</td>\n",
       "      <td>2887994266</td>\n",
       "      <td>RT @LePsylab: For science ! Un questionnaire p...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-05 19:14:45</td>\n",
       "      <td>981973435705421826</td>\n",
       "      <td>GEEZ</td>\n",
       "      <td>311289251</td>\n",
       "      <td>I lost my brova I fell deep in depression!</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date            tweet_id            handle  \\\n",
       "0 2018-04-05 19:14:48  981973445616525312            Haldol   \n",
       "1 2018-04-05 19:14:48  981973444723064832            Rick O   \n",
       "2 2018-04-05 19:14:47  981973443988996096    olivia üßùüèΩ‚Äç‚ôÄÔ∏è„Éú„Çπ   \n",
       "3 2018-04-05 19:14:47  981973443154505728  LeFrenchNeuropsy   \n",
       "4 2018-04-05 19:14:45  981973435705421826              GEEZ   \n",
       "\n",
       "                   id                                              tweet  \\\n",
       "0  816793117785542656  Currently I am on 150 mg of hydroxyzine for in...   \n",
       "1          3192532759  Integrated behavioral health for POLICE. Treat...   \n",
       "2          1321438920  RT @DevinnJay: I won‚Äôt allow depression to fuc...   \n",
       "3          2887994266  RT @LePsylab: For science ! Un questionnaire p...   \n",
       "4           311289251         I lost my brova I fell deep in depression!   \n",
       "\n",
       "  language               device  notes  notes_2  \n",
       "0       en   Twitter for iPhone    NaN      NaN  \n",
       "1       en   Twitter for iPhone    NaN      NaN  \n",
       "2       en   Twitter for iPhone    NaN      NaN  \n",
       "3       fr   Twitter Web Client    NaN      NaN  \n",
       "4       en  Twitter for Android    NaN      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154596"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how man non-distinct tweets\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter to english only\n",
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142590"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many tweets now\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".                    341\n",
       "In Music We Trust    150\n",
       "Ÿã                    135\n",
       "Aiden Hatfield       116\n",
       "üåª                    112\n",
       "Name: handle, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#any users w/lots of tweets that might skew model?\n",
    "#not any that seem too high\n",
    "df['handle'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57299"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many distinct tweets\n",
    "len(df.tweet.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make distinct tweets the df\n",
    "df = pd.DataFrame(df.tweet.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create column on 1's\n",
    "x = [1]\n",
    "x = x * len(df)\n",
    "df['target'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Currently I am on 150 mg of hydroxyzine for in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Integrated behavioral health for POLICE. Treat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @DevinnJay: I won‚Äôt allow depression to fuc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I lost my brova I fell deep in depression!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @peachesfrfr: so there i am  depression all...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  target\n",
       "0  Currently I am on 150 mg of hydroxyzine for in...       1\n",
       "1  Integrated behavioral health for POLICE. Treat...       1\n",
       "2  RT @DevinnJay: I won‚Äôt allow depression to fuc...       1\n",
       "3         I lost my brova I fell deep in depression!       1\n",
       "4  RT @peachesfrfr: so there i am  depression all...       1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in tweets\n",
    "df_2 = pd.DataFrame.from_csv('random_sample_tweets_11k2018-04-11_21-28-23.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#give index\n",
    "df_2 = df_2.reset_index()\n",
    "\n",
    "#give column name\n",
    "df_2.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#column of 0's\n",
    "x = 0\n",
    "x = x * len(df_2)\n",
    "\n",
    "df_2['target'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine dfs\n",
    "df = pd.concat([df,df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'rt @ techreview : a neural network can detect depression and mania in bipolar subjects by analyzing how they hold and tap on their smartphone‚Ä¶ '\n"
     ]
    }
   ],
   "source": [
    "#preprocess tweets\n",
    "example_text=\"\"\"'RT @techreview: A neural network can \n",
    "detect depression and mania in bipolar subjects \n",
    "by analyzing how they hold and tap on their smartphone‚Ä¶'\"\"\"\n",
    "\n",
    "# tokenize\n",
    "def tokenize_text(input_text):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    input_text: a string representing an \n",
    "    individual review\n",
    "        \n",
    "    Returns:\n",
    "    input_token: a list containing stemmed \n",
    "    tokens, with punctutations removed, for \n",
    "    an individual review\n",
    "        \n",
    "    \"\"\"\n",
    "    input_tokens=[]\n",
    "        \n",
    "    # Split sentence\n",
    "    sents=sent_tokenize(input_text)\n",
    "            \n",
    "    # Split word\n",
    "    for sent in sents:\n",
    "        input_tokens+=TreebankWordTokenizer().tokenize(sent)\n",
    "        \n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "# canonicalize\n",
    "def canonicalize_tokens(input_tokens):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    input_tokens: a list containing tokenized \n",
    "    tokens for an individual review\n",
    "    \n",
    "    Returns:\n",
    "    input_tokens: a list containing canonicalized \n",
    "    tokens for an individual review\n",
    "    \n",
    "    \"\"\"\n",
    "    input_tokens=utils.canonicalize_words(input_tokens)\n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "# preprocessor \n",
    "def preprocessor(raw_text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    raw_text: a string representing an\n",
    "    individual review\n",
    "    \n",
    "    Returns:\n",
    "    preprocessed_text: a string representing \n",
    "    a preprocessed individual review\n",
    "    \n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    tokens=tokenize_text(raw_text)\n",
    "    \n",
    "    # canonicalize\n",
    "    canonical_tokens=canonicalize_tokens(tokens)\n",
    "    \n",
    "    # rejoin string\n",
    "    preprocessed_text=(\" \").join(canonical_tokens) \n",
    "    return preprocessed_text\n",
    "\n",
    "# example data\n",
    "#input_tokens=tokenize_text(example_text)\n",
    "#print(input_tokens)\n",
    "\n",
    "#canonical_tokens=canonicalize_tokens(input_tokens)\n",
    "#print(canonical_tokens)\n",
    "\n",
    "preprocessed_text=preprocessor(example_text) \n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sklearn stopwords: 318\n",
      "number of nltk stopwords: 179\n",
      "number of total stopwords: 386\n"
     ]
    }
   ],
   "source": [
    "# examine stopwords\n",
    "\n",
    "# sklearn stopwords (frozenset)\n",
    "sklearn_stopwords=stop_words.ENGLISH_STOP_WORDS\n",
    "print(\"number of sklearn stopwords: %d\" %(len(sklearn_stopwords)))\n",
    "#print(sklearn_stopwords)\n",
    "\n",
    "# nltk stopwords (list)\n",
    "nltk_stopwords=stopwords.words(\"english\")\n",
    "print(\"number of nltk stopwords: %d\" %(len(nltk_stopwords)))\n",
    "#print(nltk_stopwords)\n",
    "\n",
    "# combined sklearn, nltk, other stopwords (set)\n",
    "total_stopwords=set(list(sklearn_stopwords.difference(set(nltk_stopwords)))+nltk_stopwords)\n",
    "\n",
    "other_stopwords=[\"DG\", \"DGDG\", \"@\", \"rt\", \"'rt\", \"'\", \":\", \"depression\"]\n",
    "for w in other_stopwords:\n",
    "    total_stopwords.add(w)\n",
    "    \n",
    "print(\"number of total stopwords: %d\" %(len(total_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['techreview', 'neural', 'network', 'detect', 'mania', 'bipolar', 'subjects', 'analyzing', 'hold', 'tap', 'smartphone‚Ä¶']\n"
     ]
    }
   ],
   "source": [
    "#look at review w/o stop words\n",
    "new_review = []\n",
    "for i in preprocessed_text.split():\n",
    "    if i in total_stopwords:\n",
    "        continue\n",
    "    else:\n",
    "        new_review.append(i)\n",
    "        \n",
    "print(new_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test set size: 18427, 4485\n",
      "\n",
      "example:\n",
      "tweet: RT @techreview: A neural network can detect depression and mania in bipolar subjects by analyzing how they hold and tap on their smartphone‚Ä¶\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "#split into test, train before sampling to belance\n",
    "# using recoded labels\n",
    "#create train, test data\n",
    "df['is_train'] = np.random.uniform(0,1, len(df)) <= .8\n",
    "\n",
    "train_data, test_data = df[df['is_train'] == True], df[df['is_train'] == False]\n",
    "\n",
    "# examine train, test shapes\n",
    "print(\"train, test set size: %d, %d\" %(len(train_data), len(test_data))) # train_data: 129023, test_data: 32256\n",
    "print(\"\")\n",
    "\n",
    "# examine train set examples\n",
    "print(\"example:\")\n",
    "print(\"tweet: %s\" %(train_data.get_value(5,'tweets')))\n",
    "print(\"label: %s\" %(train_data.get_value(5,'target')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "30",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d7d2ce97ff65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"example:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweet: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 30"
     ]
    }
   ],
   "source": [
    "print(\"example:\")\n",
    "print(\"tweet: %s\" %(train_data.get_value(30,'tweets')))\n",
    "print(\"label: %s\" %(train_data.get_value(30,'target')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build tf-idf model\n",
    "vec=TfidfVectorizer(preprocessor=preprocessor, ngram_range=(1,2), stop_words=total_stopwords, max_features=10000)\n",
    "vec_train_data=vec.fit_transform(train_data['tweets']) \n",
    "vec_test_data=vec.transform(test_data['tweets']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression f1 score: 0.799\n",
      "logistic regression accuracy score: 0.799\n",
      "logistic regression confusion matrix:\n",
      "[[1711  431]\n",
      " [ 471 1872]]\n"
     ]
    }
   ],
   "source": [
    "# train Logistic Regression\n",
    "logit=LogisticRegression(penalty='l2')\n",
    "logit.fit(vec_train_data, train_data['target'])\n",
    "pred_labels=logit.predict(vec_test_data)\n",
    "    \n",
    "# assess model\n",
    "f1=f1_score(test_data['target'], pred_labels, average=\"weighted\") \n",
    "accuracy=accuracy_score(test_data['target'], pred_labels)\n",
    "confusion=confusion_matrix(test_data['target'], pred_labels)\n",
    "print(\"logistic regression f1 score: %.3f\" %(f1))\n",
    "print(\"logistic regression accuracy score: %.3f\" %(accuracy))\n",
    "print(\"logistic regression confusion matrix:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights of words that predict depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>symptoms</th>\n",
       "      <td>2.774298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours</th>\n",
       "      <td>2.778296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imwtclothing</th>\n",
       "      <td>2.844815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>2.847842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>3.074645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>3.174490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stress</th>\n",
       "      <td>3.315463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post</th>\n",
       "      <td>3.440051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffer</th>\n",
       "      <td>3.548493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mentalhealth</th>\n",
       "      <td>3.868444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depressed</th>\n",
       "      <td>4.011660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffering</th>\n",
       "      <td>4.037699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seasonal</th>\n",
       "      <td>4.127505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nap</th>\n",
       "      <td>4.188457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crippling</th>\n",
       "      <td>4.214122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mental</th>\n",
       "      <td>4.341746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicide</th>\n",
       "      <td>4.838406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cure</th>\n",
       "      <td>5.122351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cured</th>\n",
       "      <td>6.423911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxiety</th>\n",
       "      <td>10.186968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Weights of words that predict depression\n",
       "symptoms                                      2.774298\n",
       "hours                                         2.778296\n",
       "imwtclothing                                  2.844815\n",
       "anger                                         2.847842\n",
       "feel                                          3.074645\n",
       "sadness                                       3.174490\n",
       "stress                                        3.315463\n",
       "post                                          3.440051\n",
       "suffer                                        3.548493\n",
       "mentalhealth                                  3.868444\n",
       "depressed                                     4.011660\n",
       "suffering                                     4.037699\n",
       "seasonal                                      4.127505\n",
       "nap                                           4.188457\n",
       "crippling                                     4.214122\n",
       "mental                                        4.341746\n",
       "suicide                                       4.838406\n",
       "cure                                          5.122351\n",
       "cured                                         6.423911\n",
       "anxiety                                      10.186968"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get top words\n",
    "#look at top 5 weights for each class\n",
    "#get coefficients for all features\n",
    "coef_sq = logit.coef_\n",
    "\n",
    "#get index of top 5 absolute values for each class\n",
    "weight_indx = np.argsort(coef_sq)[:, -20:]\n",
    "\n",
    "#flatten so can use to look up wieghts\n",
    "weight_indx = weight_indx.flatten()\n",
    "\n",
    "#get coefficients based on index\n",
    "weights = coef_sq[:, weight_indx]\n",
    " \n",
    "#get words that match weights based on index\n",
    "vocab = np.array(vec.get_feature_names())[weight_indx]\n",
    "\n",
    "# make table\n",
    "df = pd.DataFrame({'Weights of words that predict depression': weights[0]}\n",
    "                  , index=vocab)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try to make up an example journal\n",
    "journal = \"\"\"Nothing went right today\"\"\"\n",
    "\n",
    "vec_test_example=vec.transform([journal]) \n",
    "logit.predict(vec_test_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
